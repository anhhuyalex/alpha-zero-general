{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import logging\n",
    "from collections import deque\n",
    "import coloredlogs\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "import operator\n",
    "import functools\n",
    "import copy\n",
    "from typing import Sequence\n",
    "from pickle import Pickler, Unpickler\n",
    "import hashlib\n",
    "import sys\n",
    "import glob\n",
    "import ray\n",
    "import argparse\n",
    "\n",
    "from peyl.braid import PermTable\n",
    "from peyl import polymat, JonesCellRep, BraidGroup, GNF, Permutation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "\n",
    "# from Coach import Coach\n",
    "# from othello.OthelloGame import OthelloGame as Game\n",
    "# from othello.pytorch.NNet import NNetWrapper as nn\n",
    "import NeuralNet\n",
    "import utils \n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "coloredlogs.install(level='INFO')  # Change this to DEBUG to see more info.\n",
    "\n",
    "# Start Ray.\n",
    "ray.init(ignore_reinit_error=True)\n",
    " \n",
    "argparser = argparse.ArgumentParser()\n",
    "argparser.add_argument('--game', '-g', default='braid', choices=['braid']) \n",
    "argparser.add_argument('--max_garside_len', default=100, type=int)\n",
    "argparser.add_argument('--maxpad_of_product_matrix', default=315, type=int)\n",
    "argparser.add_argument('--bias_symlog', default=4.5, type=float)\n",
    "argparser.add_argument('--playout_cap_randomization_prob', default=0.25, type=float)\n",
    "argparser.add_argument('--do_pretrain', action=argparse.BooleanOptionalAction, default=False)\n",
    "argparser.add_argument('--startIter', default=1, type=int)\n",
    "argparser.add_argument('--numIters', default=100, type=int)\n",
    "argparser.add_argument('--numEps', default=100, type=int)\n",
    "argparser.add_argument('--tempThreshold', default=200, type=int)\n",
    "argparser.add_argument('--maxlenOfQueue', default=200000, type=int)\n",
    "argparser.add_argument('--num_jobs_at_a_time', default=5, type=int)\n",
    "argparser.add_argument('--nummaxMCTSSims', default=5, type=int)\n",
    "argparser.add_argument('--numminMCTSSims', default=1, type=int)\n",
    "argparser.add_argument('--cpuct', default=1.0, type=float)\n",
    "argparser.add_argument('--batch_size', default=256, type=int)\n",
    "argparser.add_argument('--epochs', default=10, type=int)\n",
    "argparser.add_argument('--pretrain_lr', default=2e-5, type=float)\n",
    "argparser.add_argument('--lr', default=6e-5, type=float)\n",
    "argparser.add_argument('--checkpoint', default='./temp/')\n",
    "argparser.add_argument('--load_model', default=False, type=bool)\n",
    "argparser.add_argument('--load_folder_file', default=('/dev/models/8x100x50','best.pth.tar'))\n",
    "argparser.add_argument('--numItersForTrainExamplesHistory', default=20, type=int)\n",
    "argparser.add_argument('--cuda', default=torch.cuda.is_available(), type=bool)\n",
    "argparser.add_argument('--dropout', default=0.0, type=float)\n",
    "argparser.add_argument('--mod_p', default=3, type=int)\n",
    "argparser.add_argument('--debug', action=argparse.BooleanOptionalAction, default=False)\n",
    "\n",
    "if utils.is_interactive():\n",
    "    jupyter_args = \"--numEps 1 --num_jobs_at_a_time 1 --do_pretrain --startIter 11\" \\\n",
    "            + \" --epochs 1\" \n",
    "    args = argparser.parse_args(args=jupyter_args.split())\n",
    "else:\n",
    "    args = argparser.parse_args()\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "# RAY_DEDUP_LOGS=0 ./.venv/bin/python main.py --numEps 10\n",
    "@ray.remote(num_gpus=0.2 if args.cuda == True else 0)\n",
    "class Self_play:\n",
    "    def __init__(self, policy, game, nnet, nummaxMCTSSims, numminMCTSSims, args):\n",
    "        self.policy = policy\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        self.nnet.set_eval_mode()\n",
    "        self.nummaxMCTSSims = nummaxMCTSSims\n",
    "        self.numminMCTSSims = numminMCTSSims\n",
    "        self.args = args \n",
    "        self.mcts = MCTS(self.game, self.nnet, self.args)\n",
    "\n",
    "    def executeEpisode(self):\n",
    "        \"\"\"\n",
    "        This function executes one episode of self-play, starting with player 1.\n",
    "        As the game is played, each turn is added as a training example to\n",
    "        trainExamples. The game is played till the game ends. After the game\n",
    "        ends, the outcome of the game is used to assign values to each example\n",
    "        in trainExamples.\n",
    "\n",
    "        It uses a temp=1 if episodeStep < tempThreshold, and thereafter\n",
    "        uses temp=0.\n",
    "\n",
    "        Returns:\n",
    "            trainExamples: a list of examples of the form (canonicalBoard, currPlayer, pi,v)\n",
    "                        pi is the MCTS informed policy vector, v is +1 if\n",
    "                        the player eventually won the game, else -1.\n",
    "\n",
    "        0 [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
    "        1 [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    "        2 [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
    "        3 [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    "        4 [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
    "        5 [0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0.]\n",
    "        6 [0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "        7 [0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
    "        8 [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
    "        9 [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    "        10 [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
    "        11 [0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0.]\n",
    "        12 [0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "        13 [0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
    "        14 [0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
    "        15 [0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
    "        16 [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
    "        17 [0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0.]\n",
    "        18 [0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "        19 [0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
    "        20 [0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
    "        21 [0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
    "        22 [0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
    "        23 [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        board = self.game.getInitBoard()\n",
    "        trainExamples = []\n",
    "        curPlayer = 1\n",
    "        episodeStep = 0\n",
    "\n",
    "        last_action = 0\n",
    "        projlens = [1]\n",
    "        action_list = (0,)\n",
    "        for cur_garside_len in range(self.game.getBoardSize() + 1):\n",
    "            if self.game.getGameEnded(cur_garside_len):\n",
    "                best_projlen = np.inf\n",
    "                # reward is minimum projlen attained after applying the action\n",
    "                # print (\"trainExamples\", len(trainExamples), \"projlens\", len(projlens), \"action_list\", len(action_list))\n",
    "                for i in range(len(trainExamples) - 1, -1, -1): \n",
    "                    x = trainExamples[i] \n",
    "                    if projlens[x[2]] < best_projlen:\n",
    "                        best_projlen = projlens[x[2]]\n",
    "                    trainExamples[i] = (x[0], x[1], self.game.transform_normalize_reward(best_projlen)) \n",
    "                return trainExamples, action_list[1:], projlens[1:]\n",
    "            \n",
    "            episodeStep += 1\n",
    "            canonicalBoard = self.game.getCanonicalForm(board, curPlayer)\n",
    "            temp = int(episodeStep < args.tempThreshold)\n",
    "            playout_cap_randomization = np.random.rand() < args.playout_cap_randomization_prob\n",
    "            \n",
    "            if playout_cap_randomization :\n",
    "                # On a small proportion p of\n",
    "                # turns, we perform a full search, stopping when the tree reaches a cap of N nodes, and for all\n",
    "                pi = self.mcts.getActionProb(canonicalBoard, cur_garside_len, action_list, \n",
    "                                             num_playouts = self.nummaxMCTSSims, temp=temp,\n",
    "                                             apply_Dirichlet_noise = True)\n",
    "                sym = self.game.getSymmetries(canonicalBoard, pi)\n",
    "                for b, p in sym:\n",
    "                    trainExamples.append([b, p, cur_garside_len, None])\n",
    "                action = np.random.choice(len(pi), p=pi)\n",
    "            else:\n",
    "                # other turns we perform a fast search with a much smaller cap of n < N. Only turns with a\n",
    "                # full search are recorded for training. \n",
    "                pi = self.mcts.getActionProb(canonicalBoard, cur_garside_len, action_list, \n",
    "                                            num_playouts = self.numminMCTSSims, temp=temp,\n",
    "                                            apply_Dirichlet_noise = False)\n",
    "                action = np.random.choice(len(pi), p=pi)\n",
    "            \n",
    "            try:\n",
    "                board, next_garside_len, projlen = self.game.getNextState(board, action, cur_garside_len)\n",
    "                projlens.append(projlen)\n",
    "                last_action = action \n",
    "                action_list += (action,)\n",
    "                if next_garside_len % 20 == 0: print (\"pi\", next_garside_len, pi, \"action_list\", action_list, \"projlen\", projlen, flush=True)\n",
    "            except:\n",
    "                best_projlen = np.inf\n",
    "                # reward is minimum projlen attained after applying the action\n",
    "                for i in range(len(trainExamples) - 1, -1, -1): \n",
    "                    x = trainExamples[i] \n",
    "                    if projlens[i+1] < best_projlen:\n",
    "                        best_projlen = projlens[i]\n",
    "                    trainExamples[i] = (x[0], x[1], self.game.transform_normalize_reward(best_projlen)) \n",
    "                return trainExamples, action_list[1:], projlens[1:]\n",
    "            \n",
    "class BraidGame():\n",
    "    \"\"\"\n",
    "    This class specifies the base Game class. To define your own game, subclass\n",
    "    this class and implement the functions below. This works when the game is\n",
    "    two-player, adversarial and turn-based.\n",
    "\n",
    "    Use 1 for player1 and -1 for player2.\n",
    "\n",
    "    See othello/OthelloGame.py for an example implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_garside_len, maxpad_of_product_matrix):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.max_garside_len = max_garside_len\n",
    "        self.maxpad_of_product_matrix = maxpad_of_product_matrix\n",
    "        nft = PermTable.create(n=4)\n",
    "        cell_rep = JonesCellRep(n=4, r=1, p=args.mod_p)\n",
    "        BG = BraidGroup(4)\n",
    "\n",
    "\n",
    "        mask_lookup_table = copy.deepcopy(nft.follows)\n",
    "        print (\"mask_lookup_table\", mask_lookup_table , len(mask_lookup_table) )\n",
    "        mask_lookup_table[0] =  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22] # mask for None token, 0\n",
    "        masks = np.zeros((24, 24)) \n",
    "        for i in range(24):\n",
    "            masks[i, mask_lookup_table[i]] = 1\n",
    "            print (i, masks[i])\n",
    "        self.masks = masks\n",
    "\n",
    "        gen_lookup_table = {} \n",
    "        for i, gen in enumerate(nft.divs):\n",
    "            gen_lookup_table[i] = polymat.from_matrix(cell_rep.evaluate(self.perms_to_braid(BG, [gen])), proj=True)\n",
    "            gen_lookup_table[i] = polymat.trim(gen_lookup_table[i])  \n",
    "        self.gen_lookup_table = gen_lookup_table\n",
    "\n",
    "    def perms_to_braid(self, BG: BraidGroup, perms: Sequence[Permutation]) -> GNF:\n",
    "        return functools.reduce(operator.mul, [BG.positive_lift(perm) for perm in perms], BG.id())\n",
    "\n",
    "\n",
    "    def getInitBoard(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            startBoard: a representation of the board (ideally this is the form\n",
    "                        that will be the input to your neural network)\n",
    "        \"\"\"\n",
    "        # return np.zeros(self.max_garside_len, dtype = np.int32)\n",
    "        return polymat.eye(3)\n",
    "\n",
    "    def getBoardSize(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            (x,y): a tuple of board dimensions\n",
    "        \"\"\"\n",
    "        return self.max_garside_len\n",
    "\n",
    "    def getActionSize(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            actionSize: number of all possible actions\n",
    "            allowable tokens are 0 to 22 (well 0 is only allowed to start sequences)\n",
    "            and delta is not allowed \n",
    "            but we can just return 24\n",
    "        \"\"\"\n",
    "        return 24 \n",
    "        \n",
    "\n",
    "    def getNextState(self, board, action, cur_garside_len):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "            action: action taken by current player\n",
    "            cur_garside_len: length of the braid before applying action\n",
    "\n",
    "        Returns:\n",
    "            nextBoard: board after applying action\n",
    "            next_garside_len: length of the braid after applying action\n",
    "        \"\"\"\n",
    "        try:\n",
    "            board = copy.deepcopy(board)\n",
    "            product_matrix_so_far = polymat.trim(board)\n",
    "            product_matrix_so_far = polymat.trim(polymat.mul(product_matrix_so_far, self.gen_lookup_table[action])) \n",
    "            product_matrix_so_far = np.mod(product_matrix_so_far, args.mod_p)\n",
    "            # print (\"product_matrix_so_far\", polymat.trim(board), product_matrix_so_far, product_matrix_so_far.shape, polymat.trim(product_matrix_so_far))\n",
    "            return self.getCanonicalForm(product_matrix_so_far, None), cur_garside_len + 1, polymat.projlen(product_matrix_so_far)\n",
    "        except:\n",
    "            return self.getCanonicalForm(board, None), 1e3, 1e3\n",
    "\n",
    "    def transform_normalize_reward(self, projlen):\n",
    "        \"\"\"\n",
    "        Use symlog transformation to normalize reward as in \n",
    "        https://arxiv.org/pdf/2301.04104.pdf along with bias\n",
    "        so that projlen=100 is mapped to ~0.0\n",
    "\n",
    "        negative projlen since we want to minimize projlen\n",
    "\n",
    "        Input:\n",
    "            projlen: projection length of the braid after applying action\n",
    "\n",
    "        Returns:\n",
    "            reward: reward for the action\n",
    "        \"\"\" \n",
    "        return -np.sign(projlen) * (np.log(np.abs(projlen) + 1)) + args.bias_symlog\n",
    "    \n",
    "    def getnextproductmatrix(self, product_matrix_so_far, action):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            product_matrix_so_far: product matrix of the braid before applying action\n",
    "            action: action taken by current player\n",
    "\n",
    "        Returns:\n",
    "            nextproductmatrix: product matrix of the braid after applying action\n",
    "        \"\"\"\n",
    "        \n",
    "    def getValidMoves(self, last_action):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            last_action\n",
    "\n",
    "        Returns:\n",
    "            validMoves: a binary vector of length self.getActionSize(), 1 for\n",
    "                        moves that are valid from the current board and player,\n",
    "                        0 for invalid moves\n",
    "            validMoves only depends on the current move, so we can just return the mask for the current move\n",
    "        \"\"\"\n",
    "        return self.masks[last_action]\n",
    "\n",
    "    def getGameEnded(self, cur_garside_len):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            cur_garside_len: length of the braid before applying action\n",
    "\n",
    "        Returns:\n",
    "            r: 0 if game has not ended. 1 if game ended (max garside len reached)\n",
    "               \n",
    "        \"\"\"\n",
    "        return 1 if cur_garside_len >= self.max_garside_len else 0\n",
    "\n",
    "    def getCanonicalForm(self, board, player):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "            player: current player (1 or -1)\n",
    "\n",
    "        Returns:\n",
    "            canonicalBoard: returns canonical form of board. The canonical form\n",
    "                            should be independent of player. For e.g. in chess,\n",
    "                            the canonical form can be chosen to be from the pov\n",
    "                            of white. When the player is white, we can return\n",
    "                            board as is. When the player is black, we can invert\n",
    "                            the colors and return the board.\n",
    "        \"\"\"\n",
    "        return polymat.zeropad(board, self.maxpad_of_product_matrix) \n",
    "\n",
    "    def getSymmetries(self, board, pi):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "            pi: policy vector of size self.getActionSize()\n",
    "\n",
    "        Returns:\n",
    "            symmForms: a list of [(board,pi)] where each tuple is a symmetrical\n",
    "                       form of the board and the corresponding pi vector. This\n",
    "                       is used when training the neural network from examples.\n",
    "            no symmetries known\n",
    "            # TO DO: ask Daniel\n",
    "        \"\"\"\n",
    "        return [(board,pi)]\n",
    "\n",
    "    def stringRepresentation(self, board):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "\n",
    "        Returns:\n",
    "            boardString: a quick conversion of board to a string format.\n",
    "                         Required by MCTS for hashing.\n",
    "        \"\"\" \n",
    "        b = board.astype(np.int32).data.tobytes()\n",
    "        return hashlib.sha256(b).hexdigest()\n",
    "\n",
    "class NNet(nn.Module):\n",
    "    def __init__(self, game, args):\n",
    "        # game params\n",
    "        \n",
    "        self.max_garside_len = game.getBoardSize()\n",
    "        self.action_size = game.getActionSize()\n",
    "        self.args = args\n",
    "\n",
    "        super(NNet, self).__init__() \n",
    "        self.resnet = utils.ResNet() # default: Resnet 18\n",
    "\n",
    "        embed_size = 1000\n",
    "        self.policy = nn.ModuleList ([nn.Linear(embed_size, embed_size) for i in range(6)]\n",
    "                                     + [nn.Linear(embed_size, self.action_size)])\n",
    "        self.value = nn.ModuleList ([nn.Linear(embed_size, embed_size) for i in range(6)] \n",
    "                                    + [nn.Linear(embed_size, 1)])\n",
    "                                     \n",
    "        \n",
    "    def forward(self, s):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            s: batch_size x board_x x board_y\n",
    "        Returns:\n",
    "            pi: batch_size x self.getActionSize()\n",
    "            v: batch_size x 1\n",
    "        \"\"\" \n",
    "        \n",
    "        s = 2 * s / (args.mod_p - 1) - 1 # normalize coefs of [0, mod_p - 1] to [-1, 1]\n",
    "        # print (\"s\", s) \n",
    "        if s.dim() == 3:\n",
    "            s = s.unsqueeze(0)\n",
    "\n",
    "        s = self.resnet(s)\n",
    "        pi = s\n",
    "        for pl in self.policy[:-1]:\n",
    "            pi = F.relu(pl(pi)) + pi\n",
    "        pi = self.policy[-1](pi) \n",
    "\n",
    "        v = s\n",
    "        for val in self.value[:-1]:\n",
    "            v = F.relu(val(s)) + s\n",
    "        v = self.value[-1](v)\n",
    "        \n",
    "        # print (\"pi, v\", (F.log_softmax(pi, dim=1)) , v)\n",
    "        return F.log_softmax(pi, dim=1), v\n",
    "     \n",
    "class NNetWrapper(NeuralNet.NeuralNet):\n",
    "    def __init__(self, game):\n",
    "        self.nnet = NNet(game, args)\n",
    "        # print no. of parameters \n",
    "        pytorch_total_params = sum(p.numel() for p in self.nnet.parameters())\n",
    "        print (\"pytorch_total_params\", pytorch_total_params) \n",
    "        self.max_garside_len = game.getBoardSize()\n",
    "        self.action_size = game.getActionSize()\n",
    "\n",
    "        if args.cuda:\n",
    "            self.nnet.cuda()\n",
    "\n",
    "    def set_eval_mode(self):\n",
    "        self.nnet.eval()\n",
    "\n",
    "    def train(self, examples, policy, lr):\n",
    "        \"\"\"\n",
    "        examples: list of examples, each example is of form (board, pi, v)\n",
    "        policy: \"net\" or \"random\"\n",
    "        lr: learning rate\n",
    "\n",
    "        if policy is random, only do value training\n",
    "        \"\"\"\n",
    "        # Convert examples to tensors\n",
    "        print (\"examples\", len(examples), len(examples[0]))\n",
    "        boards, pis, vs = list(zip(*examples))\n",
    "        boards = torch.tensor(np.stack(boards))\n",
    "        pis = torch.tensor(np.stack(pis))\n",
    "        vs = torch.tensor(np.stack(vs).astype(np.float64)) \n",
    "        # drop nans from vs \n",
    "        # Create a dataset from tensors\n",
    "        dataset = torch.utils.data.TensorDataset(boards, pis, vs)\n",
    "\n",
    "        # Create a data loader\n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.nnet.parameters(), \n",
    "                                    lr=lr,\n",
    "                                    weight_decay=1e-8)\n",
    "\n",
    "        for epoch in range(args.epochs):\n",
    "            print('EPOCH ::: ' + str(epoch + 1))\n",
    "            self.nnet.train()\n",
    "            pi_losses = utils.AverageMeter()\n",
    "            v_losses = utils.AverageMeter()\n",
    " \n",
    "            t = tqdm(train_loader, desc='Training Net')\n",
    "\n",
    "            for boards, target_pis, target_vs in t:\n",
    "\n",
    "            \n",
    "                optimizer.zero_grad()\n",
    "                # predict\n",
    "                if args.cuda:\n",
    "                    boards, target_pis, target_vs = boards.cuda(), target_pis.cuda(), target_vs.cuda()\n",
    "\n",
    "                # compute output\n",
    "                out_pi, out_v = self.nnet(boards)\n",
    "\n",
    "                l_pi = self.loss_pi(target_pis, out_pi)    \n",
    "                l_v = self.loss_v(target_vs, out_v)\n",
    "\n",
    "                if policy == \"random\":\n",
    "                    total_loss = l_v\n",
    "                else: \n",
    "                    total_loss = l_pi + l_v\n",
    "                # record loss\n",
    "                pi_losses.update(l_pi.item(), boards.size(0))\n",
    "                v_losses.update(l_v.item(), boards.size(0))\n",
    "                t.set_postfix(Loss_pi=pi_losses, Loss_v=v_losses)\n",
    "\n",
    "                # compute gradient and do SGD step\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        return pi_losses.avg, v_losses.avg\n",
    "\n",
    "    def predict(self, board):\n",
    "        \"\"\"\n",
    "        board: np array with board\n",
    "        \"\"\"\n",
    "        # timing\n",
    "        start = time.time()\n",
    "\n",
    "        # preparing input\n",
    "        # board = torch.LongTensor(board.astype(np.int64))\n",
    "        board = torch.FloatTensor(board.astype(np.float64)) \n",
    "        if args.cuda: board = board.contiguous().cuda()\n",
    "        self.nnet.eval()\n",
    "        with torch.no_grad():\n",
    "            pi, v = self.nnet(board)\n",
    "\n",
    "        # print('PREDICTION TIME TAKEN : {0:03f}'.format(time.time()-start))\n",
    "        return torch.exp(pi).data.cpu().numpy()[0], v.data.cpu().numpy()[0]\n",
    "\n",
    "    def loss_pi(self, targets, outputs):\n",
    "        return -torch.sum(targets * outputs) / targets.size()[0]\n",
    "\n",
    "    def loss_v(self, targets, outputs):\n",
    "        return torch.sum((targets - outputs.view(-1)) ** 2) / targets.size()[0]\n",
    "\n",
    "    def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.exists(folder):\n",
    "            print(\"Checkpoint Directory does not exist! Making directory {}\".format(folder))\n",
    "            os.mkdir(folder)\n",
    "        else:\n",
    "            print(\"Checkpoint Directory exists! \")\n",
    "        torch.save({\n",
    "            'state_dict': self.nnet.state_dict(),\n",
    "        }, filepath)\n",
    "\n",
    "    def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "        # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            raise (\"No model in path {}\".format(filepath))\n",
    "        map_location = None if args.cuda else 'cpu'\n",
    "        checkpoint = torch.load(filepath, map_location=map_location)\n",
    "        self.nnet.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "class MCTS():\n",
    "    \"\"\"\n",
    "    This class handles the MCTS tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, game, nnet, args):\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        self.args = args\n",
    "        self.Qsa = {}  # stores Q values for s,a (as defined in the paper)\n",
    "        self.Nsa = {}  # stores #times edge s,a was visited\n",
    "        self.Ns = {}  # stores #times board s was visited\n",
    "        self.Ps = {}  # stores initial policy (returned by neural net)\n",
    "\n",
    "        self.Es = {}  # stores game.getGameEnded ended for board s\n",
    "        self.Vs = {}  # stores game.getValidMoves for board s\n",
    "\n",
    "    def getActionProb(self, canonicalBoard, cur_garside_len, action_list, num_playouts, temp=1,\n",
    "                      apply_Dirichlet_noise = True):\n",
    "        \"\"\"\n",
    "        This function performs numMCTSSims simulations of MCTS starting from\n",
    "        canonicalBoard.\n",
    "\n",
    "        Params:\n",
    "            canonicalBoard: current board\n",
    "            cur_garside_len: length of the braid before applying action\n",
    "            action_list: list of actions taken by current player\n",
    "                we need last action taken by current player \n",
    "                (needed to mask out invalid moves)\n",
    "            num_playouts: number of playouts\n",
    "            temp: temp=erature\n",
    "            apply_Dirichlet_noise: whether to apply Dirichlet noise to the root node\n",
    "\n",
    "        Returns:\n",
    "            probs: a policy vector where the probability of the ith action is\n",
    "                   proportional to Nsa[(s,a)]**(1./temp)\n",
    "        \"\"\"\n",
    "        for i in range(num_playouts):\n",
    "            self.search(canonicalBoard, cur_garside_len, action_list, root=apply_Dirichlet_noise)\n",
    "\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "        s = action_list\n",
    "        counts = [self.Nsa[(s, a)] if (s, a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
    "\n",
    "        if temp == 0:\n",
    "            bestAs = np.array(np.argwhere(counts == np.max(counts))).flatten()\n",
    "            bestA = np.random.choice(bestAs)\n",
    "            probs = [0] * len(counts)\n",
    "            probs[bestA] = 1\n",
    "            return probs\n",
    "\n",
    "        counts = [x ** (1. / temp) for x in counts]\n",
    "        counts_sum = float(sum(counts))\n",
    "        probs = [x / counts_sum for x in counts]\n",
    "        return probs\n",
    "\n",
    "    def search(self, canonicalBoard, cur_garside_len, action_list, root=False):\n",
    "        \"\"\"\n",
    "        This function performs one iteration of MCTS. It is recursively called\n",
    "        till a leaf node is found. The action chosen at each node is one that\n",
    "        has the maximum upper confidence bound as in the paper.\n",
    "\n",
    "        Once a leaf node is found, the neural network is called to return an\n",
    "        initial policy P and a value v for the state. This value is propagated\n",
    "        up the search path. In case the leaf node is a terminal state, the\n",
    "        outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n",
    "        updated.\n",
    "\n",
    "        NOTE: the return values are the negative of the value of the current\n",
    "        state. This is done since v is in [-1,1] and if v is the value of a\n",
    "        state for the current player, then its value is -v for the other player.\n",
    "\n",
    "        Params:\n",
    "            canonicalBoard: current board\n",
    "            cur_garside_len: length of the braid before applying action\n",
    "            action_list: list of actions taken by current player\n",
    "                we need last action taken by current player \n",
    "                (needed to mask out invalid moves)\n",
    "        Returns:\n",
    "            v: the negative of the value of the current canonicalBoard\n",
    "        \"\"\"\n",
    "\n",
    "        # s = self.game.stringRepresentation(canonicalBoard)\n",
    "        s = action_list\n",
    "        last_action = action_list[-1] \n",
    "        print (\"executing search on\", s)\n",
    "        if self.game.getGameEnded(cur_garside_len) != 0:\n",
    "            # terminal node \n",
    "            # print (\"terminal node\", s)\n",
    "            if s not in self.Es:\n",
    "                _, v = self.nnet.predict(canonicalBoard)\n",
    "                self.Es[s] = v\n",
    "\n",
    "            return self.Es[s]\n",
    "\n",
    "        if s not in self.Ps:\n",
    "            # leaf node \n",
    "            print (\"s\", s, \"not in Ps\")\n",
    "            self.Ps[s], self.Es[s] = self.nnet.predict(canonicalBoard)\n",
    "            # if : print (\"Ps\", self.Ps[s], \"Es\", self.Es[s])\n",
    "            v = self.Es[s] # value of the leaf node\n",
    "            valids = self.game.getValidMoves(last_action) \n",
    "            # print (\"valids\", valids)\n",
    "            self.Ps[s] = self.Ps[s] * valids  # masking invalid moves\n",
    "            # print (\"masked self.Ps[s], v\", s )\n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s  # renormalize \n",
    "            else:\n",
    "                # raise ValueError(f'All valid moves were masked. {self.Ps[s]}')\n",
    "                # if all valid moves were masked make all valid moves equally probable\n",
    "\n",
    "                # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
    "                # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.   \n",
    "                \n",
    "                p, v = self.nnet.nnet(torch.FloatTensor(canonicalBoard).cuda())\n",
    "                log.error(f\"All valid moves were masked, doing a workaround {p, v}\")\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "            self.Vs[s] = valids\n",
    "            self.Ns[s] = 1\n",
    "            return v\n",
    "\n",
    "\n",
    "        valids = self.Vs[s]\n",
    "        cur_best = -float('inf')\n",
    "        best_act = -1\n",
    "        if root == True:\n",
    "            num_valids = np.sum(valids) \n",
    "            noise = np.random.dirichlet([0.3] * int(num_valids)) \n",
    "            noise_id = 0\n",
    "             \n",
    "        \n",
    "        prior_probability = {}\n",
    "\n",
    "        # pick the action with the highest upper confidence bound\n",
    "        us = []\n",
    "        netp = {}\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                if root == True: # add dirichlet noise to the prior probability for the root node\n",
    "                    netp[a] = self.Ps[s][a]\n",
    "                    prior_probability[a] = 0.75 * self.Ps[s][a] + 0.25 * noise[noise_id]\n",
    "                    noise_id += 1\n",
    "                else:\n",
    "                    prior_probability[a] = self.Ps[s][a]\n",
    "                if (s, a) in self.Qsa:\n",
    "                    u = self.Qsa[(s, a)] + self.args.cpuct * prior_probability[a] * math.sqrt(self.Ns[s]) / (\n",
    "                            1 + self.Nsa[(s, a)])\n",
    "                    if root == True: print (\"u\", s, \"a\", a, \"u\", u, \"Q\",self.Qsa[(s, a)], \"netp\", netp[a],  \"prior_probability\", prior_probability[a], \"Ns\", self.Ns[s], flush=True)\n",
    "                else:\n",
    "                    u = self.args.cpuct * (prior_probability[a]) * math.sqrt(self.Ns[s] + EPS)  # Q = 0 ?\n",
    "                    if root == True: print (\"u\", s, \"a\", a, \"u\", u, \"netp\", netp[a],\"prior_probability\", prior_probability[a], \"Ns\", self.Ns[s], flush=True)\n",
    "                us.append([a,u])\n",
    "                if u > cur_best:\n",
    "                    cur_best = u\n",
    "                    best_act = a\n",
    "                \n",
    "        sort_us_by_u = sorted(us, key=lambda x: x[1], reverse=True)\n",
    "        a = best_act \n",
    "        print (\"best_act\", s, best_act, sort_us_by_u)\n",
    "        next_s, next_cur_garside_len, _ = self.game.getNextState(canonicalBoard, a, cur_garside_len)\n",
    "        next_s = self.game.getCanonicalForm(next_s, next_cur_garside_len)\n",
    "        # print (\"recursion\", cur_garside_len, s, self.game.stringRepresentation(next_s))\n",
    "        # Search recursively\n",
    "        v = self.search(next_s, next_cur_garside_len, action_list + (a,), root=False)\n",
    "\n",
    "        if (s, a) in self.Qsa:\n",
    "            self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) / (self.Nsa[(s, a)] + 1)\n",
    "            self.Nsa[(s, a)] += 1\n",
    "\n",
    "        else:\n",
    "            self.Qsa[(s, a)] = v\n",
    "            self.Nsa[(s, a)] = 1\n",
    "\n",
    "        self.Ns[s] += 1 \n",
    "        print (\"Ps\", self.Ps.keys(), flush=True)\n",
    "        return v\n",
    "\n",
    "class Coach():\n",
    "    \"\"\"\n",
    "    This class executes the self-play + learning. It uses the functions defined\n",
    "    in Game and NeuralNet. args are specified in main.py.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, game, nnet, args):\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        # self.pnet = self.nnet.__class__(self.game)  # the competitor network\n",
    "        self.args = args\n",
    "        self.trainExamplesHistory = []  # history of examples from args.numItersForTrainExamplesHistory latest iterations\n",
    "        self.skipFirstSelfPlay = False  # can be overriden in loadTrainExamples() \n",
    "        self.best_projlen_and_seq = {i: [None, np.inf] for i in range(self.args.max_garside_len + 1)}\n",
    "     \n",
    "                \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Performs numIters iterations with numEps episodes of self-play in each\n",
    "        iteration. After every iteration, it retrains neural network with\n",
    "        examples in trainExamples (which has a maximum length of maxlenofQueue).\n",
    "        It then pits the new neural network against the old one and accepts it\n",
    "        only if it wins >= updateThreshold fraction of games.\n",
    "\n",
    "        Progressively increase the number of MCTS simulations used in the tree\n",
    "        as training progresses.\n",
    "        \"\"\"\n",
    "        if self.args.do_pretrain:\n",
    "            self.trainExamplesHistory = []\n",
    "            for i in range(0, self.args.startIter):\n",
    "                self.trainExamplesHistory.extend(self.loadTrainExamples(i))  \n",
    "\n",
    "            trainExamples = []\n",
    "            for e in self.trainExamplesHistory:\n",
    "                trainExamples.extend(e)\n",
    "            print (\"trainExamples\", len(trainExamples)) \n",
    "            pi_loss, v_loss = self.nnet.train(trainExamples[:10000], policy=\"net\", lr = self.args.pretrain_lr)\n",
    "            \n",
    "        for i in range(self.args.startIter, self.args.numIters + 1):\n",
    "            # bookkeeping\n",
    "            log.info(f'Starting Iter #{i} ...')\n",
    "            # examples of the iteration\n",
    "             \n",
    "            policy = \"net\"\n",
    "            iterationTrainExamples = []\n",
    "            nummaxMCTSSims = 2*i + self.args.nummaxMCTSSims\n",
    "            numminMCTSSims = i + self.args.numminMCTSSims\n",
    "            for j in range(0, self.args.numEps, args.num_jobs_at_a_time):\n",
    "                actors = [Self_play.remote(policy, self.game, self.nnet, nummaxMCTSSims, numminMCTSSims, self.args) for _ in range(args.num_jobs_at_a_time)]\n",
    "                iterationTrainExamples += ray.get([actor.executeEpisode.remote() for actor in actors])\n",
    "\n",
    "            iterationTrainExamples, action_lists, projlens = zip(*iterationTrainExamples) \n",
    "            # iterationTrainExamples: list of list of examples of the form (canonicalBoard, currPlayer, pi,v)\n",
    "            iterationTrainExamples = [item for sublist in iterationTrainExamples for item in sublist]\n",
    "            print (\"iterationTrainExamples\", len(iterationTrainExamples))\n",
    "            print (\"action_lists\", action_lists, flush=True)\n",
    "            print (\"projlens\", projlens, flush=True)\n",
    "            for (action_list, projlen) in zip(action_lists, projlens):\n",
    "                 \n",
    "                for l in range(0, len(action_list)):\n",
    "                    if projlen[l] < self.best_projlen_and_seq[l+1][1]:\n",
    "                        self.best_projlen_and_seq[l+1] = [action_list[:l], projlen[l]] \n",
    "                    if projlen[l] == 1:\n",
    "                        print (\"found kernel\", action_list[:l])\n",
    "            \n",
    "\n",
    "            print (\"self.best_projlen_and_seq\", self.best_projlen_and_seq)\n",
    "\n",
    "            self.saveTrainExamples(iterationTrainExamples, i - 1)\n",
    "            iterationTrainExamples = self.loadTrainExamples(i - 1)\n",
    "            self.trainExamplesHistory.extend(iterationTrainExamples)\n",
    "           \n",
    "            if len(self.trainExamplesHistory) > self.args.numItersForTrainExamplesHistory:\n",
    "                log.warning(\n",
    "                    f\"Removing the oldest entries in trainExamples. len(trainExamplesHistory) = {len(self.trainExamplesHistory)}\")\n",
    "                self.trainExamplesHistory = self.trainExamplesHistory[-self.args.numItersForTrainExamplesHistory:]\n",
    "                 \n",
    "\n",
    "            trainExamples = []\n",
    "            for e in self.trainExamplesHistory:\n",
    "                trainExamples.extend(e)\n",
    "            print (\"trainExamples\", len(trainExamples))\n",
    "\n",
    "            # training new network, keeping a copy of the old one\n",
    "            self.nnet.save_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')\n",
    "            pi_loss, v_loss = self.nnet.train(trainExamples, policy=policy, lr = self.args.lr) \n",
    "\n",
    "            self.logtrainingmetrics(action_lists, projlens, pi_loss, v_loss, i - 1)\n",
    "\n",
    "    def getCheckpointFile(self, iteration):\n",
    "        return 'checkpoint_epoch_' + str(iteration) + f'_mod_p_{args.mod_p}'\n",
    "\n",
    "    def saveTrainExamples(self, iterationTrainExamples, iteration):\n",
    "        if args.debug: return\n",
    "        folder = self.args.checkpoint\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        filename = os.path.join(folder, self.getCheckpointFile(iteration) + f\"_{time.time()}.examples\")\n",
    "        with open(filename, \"wb+\") as f:\n",
    "            Pickler(f).dump(iterationTrainExamples)\n",
    "        f.closed\n",
    "\n",
    "    def logtrainingmetrics(self, action_lists, projlens, pi_loss, v_loss, iteration):\n",
    "        if args.debug: return\n",
    "        folder = self.args.checkpoint \n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        filename = os.path.join(folder, f\"iteration_{iteration}_{time.time()}.actions_projlens\") \n",
    "        action_lists_projlens = {\n",
    "            \"action_lists\": action_lists,\n",
    "            \"projlens\": projlens, \n",
    "            \"pi_loss\": pi_loss,\n",
    "            \"v_loss\": v_loss,\n",
    "        }\n",
    "        with open(filename, \"wb+\") as f:\n",
    "            Pickler(f).dump(action_lists_projlens)\n",
    "        f.closed\n",
    "\n",
    "    def loadTrainExamples(self, iteration):\n",
    "        folder = self.args.checkpoint\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        fs = glob.glob(os.path.join(folder, self.getCheckpointFile(iteration) + \"*.examples\"))\n",
    "\n",
    "        iterationTrainExamples = []\n",
    "        for filename in fs:\n",
    "            with open(filename, \"rb\") as f:\n",
    "                iterationTrainExamples.append(Unpickler(f).load())\n",
    "        log.info(f'Loading done for iteration {iteration}!')\n",
    "        return iterationTrainExamples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    log.info('Loading %s...', BraidGame.__name__)\n",
    "    g = BraidGame(args.max_garside_len, args.maxpad_of_product_matrix)\n",
    "\n",
    "    # log.info('Loading %s...', nn.__name__)\n",
    "    nnet = NNetWrapper(g)\n",
    "\n",
    "    # if args.load_model:\n",
    "    #     log.info('Loading checkpoint \"%s/%s\"...', args.load_folder_file[0], args.load_folder_file[1])\n",
    "    #     nnet.load_checkpoint(args.load_folder_file[0], args.load_folder_file[1])\n",
    "    # else:\n",
    "    #     log.warning('Not loading a checkpoint!')\n",
    "\n",
    "    log.info('Loading the Coach...')\n",
    "    c = Coach(g, nnet, args)\n",
    "\n",
    "    # if args.load_model:\n",
    "    #     log.info(\"Loading 'trainExamples' from file...\")\n",
    "    #     c.loadTrainExamples()\n",
    "\n",
    "    log.info('Starting the learning process ðŸŽ‰')\n",
    "    c.learn()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (alphazerobraids)",
   "language": "python",
   "name": "alphazerobraids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
